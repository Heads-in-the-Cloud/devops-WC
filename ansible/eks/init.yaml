---
- hosts: localhost
  connection: local
  gather_facts: false
  vars_files:
    - eks_vars.yaml
    
  tasks:
    - name: Get Environment
      set_fact:
        environment: "{{ ENVIRONMENT }}"
      ##########################
      ######  Get VPC ID  ######
      ##########################
    - name: Getting VPC info
      ec2_vpc_net_info:
        region: "{{ REGION }}"
        filters:
          "tag:Name": "{{VPC_NAME}}"
      register: vpc
      tags: ['create', 'test']
      

    - name: Setting vpc_id fact
      set_fact:
        VPC_ID: "{{ vpc.vpcs[0].vpc_id }}"
      tags: ['create', 'test']

      ##############################
      ######  Get Subnet IDs  ######
      ##############################
    - name: Getting private subnets info
      ec2_vpc_subnet_info:
        region: "{{ REGION }}"
        filters:
          vpc-id: "{{ VPC_ID }}"
          "tag:kubernetes.io/role/internal-elb": "1"
      loop:
        - subnet1
        - subnet2
      register: subnets
      tags: ['create', 'test']

    - name: debug
      debug: msg="{{ subnets }}"


    - name: Setting subnet1 fact
      set_fact:
        SUBNET1: "{{ subnets.results[0].subnets[0].id }}"
      tags: ['create', 'test']

    - name: Setting subnet2 fact
      set_fact:
        SUBNET2: "{{ subnets.results[0].subnets[1].id }}"
      tags: ['create', 'test']

      ##############################
      ######  Create Cluster  ######
      ##############################
     
    - name: Create EKS cluster
      shell: |
        eksctl create cluster --name="{{ CLUSTER_NAME }}" --region="{{ REGION }}" --fargate \
        --vpc-private-subnets="{{SUBNET1}},{{SUBNET2}}"
      tags: ['create', 'test']
      ##############################
      ######  Set Up Cluster  ######
      ##############################

    - name: Associate Open ID Provider
      shell: |
        eksctl utils associate-iam-oidc-provider --cluster="{{ CLUSTER_NAME }}" --approve
      tags: ['create']
  
    - name: Switch context to cluster
      shell: |
        aws eks --region "{{ REGION }}" update-kubeconfig --name "{{ CLUSTER_NAME }}"
      tags: ['create', 'destroy', 'update', 'logging']

      #####################################
      ##### Set up Ingress controller #####
      #####################################
    - name: Create ALB Ingress Controller
      shell: |
        kubectl apply -f "https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.4/docs/examples/rbac-role.yaml"
        eksctl create iamserviceaccount \
          --name=alb-ingress-controller \
          --namespace=kube-system \
          --cluster="{{ CLUSTER_NAME }}" \
          --attach-policy-arn="arn:aws:iam::{{ AWS_ACCOUNT_ID }}:policy/ALBIngressControllerIAMPolicy" \
          --override-existing-serviceaccounts \
          --approve

        curl -sS "https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.4/docs/examples/alb-ingress-controller.yaml" \
            | sed "s/# - --cluster-name=devCluster/- --cluster-name={{ CLUSTER_NAME }}/g" \
            | sed "s/# - --aws-vpc-id=vpc-xxxxxx/- --aws-vpc-id={{ VPC_ID }}/g" \
            | sed "s/# - --aws-region=us-west-1/- --aws-region={{ REGION }}/g" \
            | kubectl apply -f -
      tags: ['create']

      #################################
      #### Create Secrets from SSM ####
      #################################

    - name: Extract Secrets from SSM and create Kubernetes Secrets
      shell: | 
        secrets="{{ lookup('aws_secret', SECRET_NAME, region=REGION, nested=true) }}"
        chmod +x parser.py
        db_user=$(python parser.py $secrets "{{ DB_USER }}" 2>&1 > /dev/null)
        db_password=$(python parser.py $secrets "{{ DB_PASSWORD }}" 2>&1 > /dev/null)
        db_host=$(python parser.py $secrets "{{ DB_HOST }}" 2>&1 > /dev/null)
        secret_key=$(python parser.py $secrets "{{ SECRET_KEY }}" 2>&1 > /dev/null)

        kubectl create secret generic db-info \
          --from-literal=db_user=$db_user \
          --from-literal=db_host=$db_host \
          --from-literal=db_user_password=$db_password
        kubectl create secret generic jwt-secret \
          --from-literal=secret_key=$secret_key
      tags: ['create', 'update']

      ################################
      ### kubectl create resources ###
      ################################

    - name: Create Kubernetes Resources
      shell: |
        kubectl apply -f service.yaml -f ingress.yaml
        sed -e 's/$AWS_REGION/'"{{REGION}}"'/g' -e 's/$AWS_ACCOUNT_ID/'"{{AWS_ACCOUNT_ID}}"'/g' -e 's/$RECORD_NAME/'"http:\/\/{{ ENVIRONMENT }}-{{RECORD_NAME}}"'/g' deployment.yaml | kubectl apply -f -
      args:
        chdir: "{{ EKS_LOCATION }}"
      tags: ['create', 'update']
      
    # - name: Set up CloudWatch Logs
    #   shell: |
    #     kubectl apply -f namespace.yaml
    #     sed -e 's/$AWS_REGION/'"{{REGION}}"'/g' cloudwatch.yaml | kubectl apply -f -
    #     curl -o permissions.json https://raw.githubusercontent.com/aws-samples/amazon-eks-fluent-logging-examples/mainline/examples/fargate/cloudwatchlogs/permissions.json
    #     aws iam create-policy --policy-name eks-fargate-logging-policy --policy-document file://permissions.json
    #     export PodRole=$(aws eks describe-fargate-profile --cluster-name "{{ CLUSTER_NAME }}" --fargate-profile-name fp-default --query 'fargateProfile.podExecutionRoleArn' | sed -n 's/^.*role\/\(.*\)".*$/\1/ p')
    #     aws iam attach-role-policy \
    #       --policy-arn "arn:aws:iam::{{ AWS_ACCOUNT_ID }}:policy/FluentBitEKSFargate" \
    #       --role-name ${PodRole}
    #     echo $PodRole

    #     eksctl utils update-cluster-logging \
    #     --enable-types all \
    #     --region "{{ REGION }}" \
    #     --cluster "{{ CLUSTER_NAME }}" \
    #     --approve

    #   args:
    #     chdir: "{{ EKS_LOCATION }}"
    #   tags: ['create', 'update']
    
    - name: Set up CloudWatch Logs with Fluent Bit
      shell: |
        kubectl apply -f https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/cloudwatch-namespace.yaml
        ClusterName="{{ CLUSTER_NAME }}"
        RegionName="{{ REGION }}"
        FluentBitHttpPort='2020'
        FluentBitReadFromHead='Off'
        [[ ${FluentBitReadFromHead} = 'On' ]] && FluentBitReadFromTail='Off'|| FluentBitReadFromTail='On'
        [[ -z ${FluentBitHttpPort} ]] && FluentBitHttpServer='Off' || FluentBitHttpServer='On'
        kubectl create configmap fluent-bit-cluster-info \
        --from-literal=cluster.name=${ClusterName} \
        --from-literal=http.server=${FluentBitHttpServer} \
        --from-literal=http.port=${FluentBitHttpPort} \
        --from-literal=read.head=${FluentBitReadFromHead} \
        --from-literal=read.tail=${FluentBitReadFromTail} \
        --from-literal=logs.region=${RegionName} -n amazon-cloudwatch
        kubectl apply -f https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/fluent-bit/fluent-bit.yaml
      tags: ['create', 'logging']


    - name: Scale Prod Resources
      shell: |
        kubectl scale deployment/users-deployment --replicas="{{ USERS_NUM_POD }}"
        kubectl scale deployment/flights-deployment --replicas="{{ FLIGHTS_NUM_POD }}"
        kubectl scale deployment/bookings-deployment --replicas="{{ BOOKINGS_NUM_POD }}"
        kubectl scale deployment/frontend-deployment --replicas="{{ FRONTEND_NUM_POD }}"
      when: environment == "prod"
      tags: ['create', 'update']

      ##############################
      #########  Route 53  #########
      ##############################

    - name: Set up Route53
      shell: | 
        sleep 200
        # https://stackoverflow.com/a/70108500
        DNS=$(kubectl get ingress utopia-ingress --output=jsonpath='{.status.loadBalancer.ingress[0].hostname}')        
        aws route53 change-resource-record-sets --hosted-zone-id "{{ HOSTED_ZONE }}" --change-batch '
          {
              "Comment": "Testing creating a record set"
              ,"Changes": [{
                  "Action"              : "CREATE"
                  ,"ResourceRecordSet"  : {
                  "Name"              : "{{ ENVIRONMENT }}-{{ RECORD_NAME }}"
                  ,"Type"             : "CNAME"
                  ,"TTL"              : 120
                  ,"ResourceRecords"  : [{
                      "Value"         : "'$DNS'"
                  }]
                  }
              }]
              }'
      tags: ['create']

      ##############################
      ##########  Delete  ##########
      ##############################

    - name: Delete load balancer
      ignore_errors: yes
      shell: | 
        kubectl delete ingress utopia-ingress
      tags: ['destroy']

    - name: Delete Route53
      ignore_errors: yes
      shell: | 
        DNS=$(kubectl get ingress utopia-ingress --output=jsonpath='{.status.loadBalancer.ingress[0].hostname}')
        aws route53 change-resource-record-sets --hosted-zone-id "{{ HOSTED_ZONE }}" --change-batch '
        {
            "Comment": "Testing creating a record set"
            ,"Changes": [{
              "Action"              : "DELETE"
              ,"ResourceRecordSet"  : {
                "Name"              : "{{ ENVIRONMENT }}-{{ RECORD_NAME }}"
                ,"Type"             : "CNAME"
                ,"TTL"              : 120
                ,"ResourceRecords"  : [{
                    "Value"         : "'$DNS'"
                }]
              }
            }]
          }'
      tags: ['destroy']
      
    - name: Delete EKS cluster
      shell: | 
          eksctl delete cluster "{{ CLUSTER_NAME }}" --region "{{ REGION }}"
      tags: ['destroy']

