---
- hosts: localhost
  connection: local
  gather_facts: false
  vars_files:
    - eks_vars.yaml
    
  tasks:
    - name: Get Environment
      set_fact:
        environment: "{{ ENVIRONMENT }}"
      ##########################
      ######  Get VPC ID  ######
      ##########################
    - name: Getting VPC info
      ec2_vpc_net_info:
        region: "{{ REGION }}"
        filters:
          "tag:Name": "{{VPC_NAME}}"
      register: vpc
      tags: ['create', 'test']
      

    - name: Setting vpc_id fact
      set_fact:
        VPC_ID: "{{ vpc.vpcs[0].vpc_id }}"
      tags: ['create', 'test']

      ##############################
      ######  Get Subnet IDs  ######
      ##############################
    - name: Getting private subnets info
      ec2_vpc_subnet_info:
        region: "{{ REGION }}"
        filters:
          vpc-id: "{{ VPC_ID }}"
          "tag:kubernetes.io/role/internal-elb": "1"
      loop:
        - subnet1
        - subnet2
      register: subnets
      tags: ['create', 'test']

    - name: debug
      debug: msg="{{ subnets }}"


    - name: Setting subnet1 fact
      set_fact:
        SUBNET1: "{{ subnets.results[0].subnets[0].id }}"
      tags: ['create', 'test']

    - name: Setting subnet2 fact
      set_fact:
        SUBNET2: "{{ subnets.results[0].subnets[1].id }}"
      tags: ['create', 'test']

      ##############################
      ######  Create Cluster  ######
      ##############################
     
    - name: Create EKS cluster
      shell: |
        eksctl create cluster --name="{{ CLUSTER_NAME }}" --region="{{ REGION }}" --fargate \
        --vpc-private-subnets="{{SUBNET1}},{{SUBNET2}}"
      tags: ['create', 'test']
      ##############################
      ######  Set Up Cluster  ######
      ##############################

    - name: Associate Open ID Provider
      shell: |
        eksctl utils associate-iam-oidc-provider --cluster="{{ CLUSTER_NAME }}" --approve
      tags: ['create']
  
    - name: Switch context to cluster
      shell: |
        aws eks --region us-west-2 update-kubeconfig --name "{{ CLUSTER_NAME }}"
      tags: ['create', 'destroy', 'update', 'try']

      #####################################
      ##### Set up Ingress controller #####
      #####################################
    - name: Create ALB Ingress Controller
      shell: |
        kubectl apply -f "https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.4/docs/examples/rbac-role.yaml"
        eksctl create iamserviceaccount \
          --name=alb-ingress-controller \
          --namespace=kube-system \
          --cluster="{{ CLUSTER_NAME }}" \
          --attach-policy-arn="arn:aws:iam::{{ AWS_ACCOUNT_ID }}:policy/ALBIngressControllerIAMPolicy" \
          --override-existing-serviceaccounts \
          --approve

        curl -sS "https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.4/docs/examples/alb-ingress-controller.yaml" \
            | sed "s/# - --cluster-name=devCluster/- --cluster-name={{ CLUSTER_NAME }}/g" \
            | sed "s/# - --aws-vpc-id=vpc-xxxxxx/- --aws-vpc-id={{ VPC_ID }}/g" \
            | sed "s/# - --aws-region=us-west-1/- --aws-region={{ REGION }}/g" \
            | kubectl apply -f -
      tags: ['create']

      #################################
      #### Create Secrets from SSM ####
      #################################

    - name: Extract Secrets from SSM and create Kubernetes Secrets
      shell: | 
        secrets="{{ lookup('aws_secret', SECRET_NAME, region=REGION, nested=true) }}"
        chmod +x parser.py
        db_user=$(python parser.py $secrets "{{ DB_USER }}" 2>&1 > /dev/null)
        db_password=$(python parser.py $secrets "{{ DB_PASSWORD }}" 2>&1 > /dev/null)
        db_host=$(python parser.py $secrets "{{ DB_HOST }}" 2>&1 > /dev/null)
        secret_key=$(python parser.py $secrets "{{ SECRET_KEY }}" 2>&1 > /dev/null)

        kubectl create secret generic db-info \
          --from-literal=db_user=$db_user \
          --from-literal=db_host=$db_host \
          --from-literal=db_user_password=$db_password
        kubectl create secret generic jwt-secret \
          --from-literal=secret_key=$secret_key
      tags: ['create', 'update']

      ################################
      ### kubectl create resources ###
      ################################

    - name: Create Kubernetes Resources
      shell: |
        kubectl apply -f service.yaml -f ingress.yaml
        sed -e 's/$AWS_REGION/'"{{REGION}}"'/g' -e 's/$AWS_ACCOUNT_ID/'"{{AWS_ACCOUNT_ID}}"'/g' -e 's/$RECORD_NAME/'"http:\/\/{{ ENVIRONMENT }}-{{RECORD_NAME}}"'/g' deployment.yaml | kubectl apply -f -
      args:
        chdir: "{{ EKS_LOCATION }}"
      tags: ['create', 'update']
      
    - name: Set up CloudWatch Logs
      shell: |
        kubectl apply -f namespace.yaml
        sed -e 's/$AWS_REGION/'"{{REGION}}"'/g' cloudwatch.yaml | kubectl apply -f -
        curl -o permissions.json https://raw.githubusercontent.com/aws-samples/amazon-eks-fluent-logging-examples/mainline/examples/fargate/cloudwatchlogs/permissions.json
        aws iam create-policy --policy-name eks-fargate-logging-policy --policy-document file://permissions.json
        export PodRole=$(aws eks describe-fargate-profile --cluster-name "{{ CLUSTER_NAME }}" --fargate-profile-name fp-default --query 'fargateProfile.podExecutionRoleArn' | sed -n 's/^.*role\/\(.*\)".*$/\1/ p')
        aws iam attach-role-policy \
          --policy-arn "arn:aws:iam::{{ AWS_ACCOUNT_ID }}:policy/FluentBitEKSFargate" \
          --role-name ${PodRole}
        echo $PodRole
      args:
        chdir: "{{ EKS_LOCATION }}"
      tags: ['create', 'update', 'try']

      ##############################
      #########  Route 53  #########
      ##############################
    
    - name: Scale Prod Resources
      shell: |
        kubectl scale users-deployment --replicas="{{ USERS_NUM_POD }}"
        kubectl scale flights-deployment --replicas="{{ FLIGHTS_NUM_POD }}"
        kubectl scale bookings-deployment --replicas="{{ BOOKINGS_NUM_POD }}"
        kubectl scale frontend-deployment --replicas="{{ FRONTEND_NUM_POD }}"
      when: environment == "prod"
      tags: ['create', 'update']

    - name: Set up Route53
      shell: | 
        sleep 200
        # https://stackoverflow.com/a/70108500
        DNS=$(kubectl get ingress utopia-ingress --output=jsonpath='{.status.loadBalancer.ingress[0].hostname}')        
        aws route53 change-resource-record-sets --hosted-zone-id "{{ HOSTED_ZONE }}" --change-batch '
          {
              "Comment": "Testing creating a record set"
              ,"Changes": [{
                  "Action"              : "CREATE"
                  ,"ResourceRecordSet"  : {
                  "Name"              : "{{ ENVIRONMENT }}-{{ RECORD_NAME }}"
                  ,"Type"             : "CNAME"
                  ,"TTL"              : 120
                  ,"ResourceRecords"  : [{
                      "Value"         : "'$DNS'"
                  }]
                  }
              }]
              }'
      tags: ['create']

      ##############################
      ##########  Delete  ##########
      ##############################

    - name: Delete load balancer
      ignore_errors: yes
      shell: | 
        kubectl delete ingress utopia-ingress
      tags: ['destroy']

    - name: Delete Route53
      ignore_errors: yes
      shell: | 
        DNS=$(kubectl get ingress utopia-ingress --output=jsonpath='{.status.loadBalancer.ingress[0].hostname}')
        aws route53 change-resource-record-sets --hosted-zone-id "{{ HOSTED_ZONE }}" --change-batch '
        {
            "Comment": "Testing creating a record set"
            ,"Changes": [{
              "Action"              : "DELETE"
              ,"ResourceRecordSet"  : {
                "Name"              : "{{ ENVIRONMENT }}-{{ RECORD_NAME }}"
                ,"Type"             : "CNAME"
                ,"TTL"              : 120
                ,"ResourceRecords"  : [{
                    "Value"         : "'$DNS'"
                }]
              }
            }]
          }'
      tags: ['destroy']
      
    - name: Delete EKS cluster
      shell: | 
          eksctl delete cluster "{{ CLUSTER_NAME }}" --region "{{ REGION }}"
      tags: ['destroy']

